# MLT-Papers
Awesome papers in machine learning theory.

## Contents
- [Learning Theory](#learning-theory)
- [Kernel Methods and Regularization](#kernel-regularization)
- [Theory of Neural Networks](#nn-theory)


<a name="learning-theory" />

## Learning Theory

1. **Algorithmic Stability and Generalization Performance.**
    - NIPS 2001.
    - *Olivier Bousquet, Andre Elisseeff.*
    - [paper](https://papers.nips.cc/paper/1854-algorithmic-stability-and-generalization-performance.pdf)

1. **A Generalized Representer Theorem.**
    - COLT 2001.
    - *Bernhard Scholkopf, Ralf Herbrich, Alex J. Smola.*
    - [paper](https://www.cise.ufl.edu/class/cap6617fa15/Readings/SchHerSmo01.pdf)

1. **Stability and Generalization.**
    - JMLR 2002.
    - *Olivier Bousquet, Andre Elisseeff.*
    - [paper](http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf)
    
1. **Statistical Behavior and Consistency of Classification Methods based on Convex Risk Minimization.**
    - Annals of Statistics 2004.
    - *Tong Zhang.*
    - [paper](https://projecteuclid.org/download/pdf_1/euclid.aos/1079120130)
    
1. **Theory of Classification: A Survey of Some Recent Advances.**
    - ESAIM: Probability and Statistics 2005.
    - *Stephane Boucheron, Olivier Bousquet, Gabor Lugosi.*
    - [paper](https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf)
    
1. **Learning Theory: Stability is Sufficient for Generalization and Necessary and Sufficient for Consistency of Empirical Risk Minimization.**
    - Advances in Computational Mathematics 2006.
    - *Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, Ryan Rifkin.*
    - [paper](https://link.springer.com/content/pdf/10.1007/s10444-004-7634-z.pdf)
    
1. **Rademacher Complexity Bounds for Non-I.I.D. Processes.**
    - NIPS 2008.
    - *Mehryar Mohri, Afshin Rostamizadeh.*
    - [paper](https://papers.nips.cc/paper/3489-rademacher-complexity-bounds-for-non-iid-processes.pdf)


<a name="kernel-regularization" />

## Kernel Methods and Regularization

1. **Think Globally, Fit Locally Under the Manifold Setup: Asymptotic Analysis of Locally Linear Embedding.**
    - Annals of Statistics 2018.
    - *Hau-Tieng Wu, Nan Wu.*
    - [paper](https://projecteuclid.org/download/pdfview_1/euclid.aos/1536631291)

1. **Convergence Analysis of Tikhonov Regularization for Non-Linear Statistical Inverse Learning Problems.**
    - arXiv 2019.
    - *Abhishake Rastogi, Gilles Blanchard, Peter Math√©.*
    - [paper](https://arxiv.org/pdf/1902.05404.pdf)
    
    
 <a name="nn-theory" />
 
 ## Theory of Neural Networks
 
 1. **Strong Universal Consistency of Neural Network Classifiers.**
    - IEEE Transactions on Information Theory 1993.
    - *AndrAs Farag, GAbor Lugosi.*
    - [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243433)
    
1. **Benefits of Depth in Neural Networks.**
    - COLT 2016.
    - *Matus Telgarsky.*
    - [paper](https://arxiv.org/pdf/1602.04485.pdf)

1. **Understanding Deep Learning Requires Rethinking Generalization.**
    - ICLR 2017.
    - *Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals.*
    - [paper](https://arxiv.org/pdf/1611.03530.pdf)
    
1. **Convergence Analysis of Two-layer Neural Networks with ReLU Activation.**
    - arXiv 2017.
    - *Yuanzhi Li, Yang Yuan.*
    - [paper](https://arxiv.org/pdf/1705.09886.pdf)
    
1. **To Understand Deep Learning We Need to Understand Kernel Learning.**
    - ICML 2018.
    - *Mikhail Belkin, Siyuan Ma, Soumik Mandal.*
    - [paper](https://arxiv.org/pdf/1802.01396.pdf)
    
1. **Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks.**
    - arXiv 2019.
    - *Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, Zheng Ma.*
    - [paper](https://arxiv.org/pdf/1905.10264.pdf)
    
1. **Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers.**
    - NIPS 2019.
    - *Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang.*
    - [paper](https://arxiv.org/pdf/1811.04918.pdf)
    
1. **Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent.**
    - arXiv 2020.
    - *David Holzmuller, Ingo Steinwart.*
    - [paper](https://arxiv.org/pdf/2002.04861.pdf)
    
